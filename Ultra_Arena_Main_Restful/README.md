# Ultra Arena REST API Server

A high-performance REST API server built with FastAPI that provides web-based access to the Ultra Arena document processing platform. Features comprehensive API endpoints, real-time monitoring, and scalable architecture.

## üèóÔ∏è Architecture Overview

```mermaid
graph TB
    subgraph "Client Layer"
        A[Web Client]
        B[Mobile App]
        C[CLI Client]
    end
    
    subgraph "API Gateway"
        D[FastAPI Server]
        E[Request Router]
        F[Authentication]
    end
    
    subgraph "Processing Layer"
        G[Request Processor]
        H[Config Assembler]
        I[Profile Manager]
    end
    
    subgraph "Core Engine"
        J[Ultra Arena Main]
        K[LLM Clients]
        L[Processing Strategies]
    end
    
    subgraph "Monitoring"
        M[Performance Monitor]
        N[Metrics Collector]
        O[Real-time Dashboard]
    end
    
    A --> D
    B --> D
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
    H --> I
    I --> J
    J --> K
    J --> L
    J --> M
    M --> N
    N --> O
```

## üìÅ Directory Structure

| Directory | Purpose | Key Components |
|-----------|---------|----------------|
| **`server/`** | Core server implementation | `server.py`, `request_processor.py` |
| **`server/config_assemblers/`** | Configuration management | `base_config_assembler.py`, `config_models.py` |
| **`run_profiles/`** | API profiles and configurations | `default_profile_restful/` |
| **`performance_measure/`** | Performance monitoring | `core_monitor.py`, `rest_wrapper.py` |
| **`cursor_gen/`** | Generated documentation | Implementation notes |

## üöÄ Quick Start

### Installation
```bash
pip install -r requirements.txt
```

### Start Server
```bash
# Development mode
python server.py

# Production mode
uvicorn server:app --host 0.0.0.0 --port 8000 --workers 4
```

### Server Configuration
```python
# Default settings
HOST = "0.0.0.0"
PORT = 8000
DEBUG = False
WORKERS = 4
```

## üîå API Endpoints

### Core Endpoints

| Endpoint | Method | Description | Parameters |
|----------|--------|-------------|------------|
| `/health` | GET | Server health check | None |
| `/combos` | GET | List available combos | None |
| `/combos/{combo_id}` | GET | Get specific combo | `combo_id` |
| `/process-combo` | POST | Process combo with files | `combo_id`, `files` |
| `/process-files` | POST | Process files directly | `strategy`, `provider`, `files` |
| `/profiles` | GET | List available profiles | None |
| `/profiles/{profile_id}` | GET | Get profile details | `profile_id` |

### Health Check
```bash
curl -X GET "http://localhost:8000/health"
```

**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2024-01-15T10:30:00Z",
  "version": "1.0.0",
  "uptime": 3600
}
```

### Get Available Combos
```bash
curl -X GET "http://localhost:8000/combos"
```

**Response:**
```json
{
  "combos": [
    {
      "id": "benchmark_combo",
      "name": "Benchmark Processing",
      "description": "Standard benchmark processing combo",
      "strategies": ["direct_file", "image_first"],
      "providers": ["claude", "gpt"]
    }
  ]
}
```

## üì§ Request/Response Examples

### Process Combo Request
```bash
curl -X POST "http://localhost:8000/process-combo" \
  -H "Content-Type: multipart/form-data" \
  -F "combo_id=benchmark_combo" \
  -F "files=@document1.pdf" \
  -F "files=@document2.pdf"
```

**Request Body:**
```json
{
  "combo_id": "benchmark_combo",
  "files": ["document1.pdf", "document2.pdf"],
  "options": {
    "timeout": 300,
    "max_concurrent": 4
  }
}
```

**Response:**
```json
{
  "request_id": "req_123456789",
  "status": "processing",
  "estimated_time": 120,
  "files_count": 2,
  "combo": {
    "id": "benchmark_combo",
    "strategies": ["direct_file", "image_first"]
  }
}
```

### Process Files Directly
```bash
curl -X POST "http://localhost:8000/process-files" \
  -H "Content-Type: multipart/form-data" \
  -F "strategy=direct_file" \
  -F "provider=claude" \
  -F "files=@document.pdf"
```

## üìä API Models

### Request Models
```python
class ProcessComboRequest(BaseModel):
    combo_id: str
    files: List[UploadFile]
    options: Optional[ProcessOptions] = None

class ProcessFilesRequest(BaseModel):
    strategy: str
    provider: str
    files: List[UploadFile]
    options: Optional[ProcessOptions] = None

class ProcessOptions(BaseModel):
    timeout: Optional[int] = 300
    max_concurrent: Optional[int] = 4
    output_format: Optional[str] = "json"
```

### Response Models
```python
class ProcessResponse(BaseModel):
    request_id: str
    status: str
    estimated_time: Optional[int]
    files_count: int
    results: Optional[List[ProcessResult]]

class ProcessResult(BaseModel):
    file_name: str
    status: str
    processing_time: float
    result: Optional[Dict]
    error: Optional[str]
```

## üîß Configuration

### Profile Configuration
```python
# run_profiles/default_profile_restful/profile_config.py
DEFAULT_STRATEGY = "direct_file"
DEFAULT_PROVIDER = "claude"
DEFAULT_TIMEOUT = 300
MAX_CONCURRENT_STRATEGIES = 4
API_RATE_LIMIT = 100  # requests per minute
```

### API Keys Configuration
```python
# run_profiles/default_profile_restful/config_api_keys.py
CLAUDE_API_KEY = "your-claude-api-key"
OPENAI_API_KEY = "your-openai-api-key"
```

## üìà Performance Monitoring

### Real-time Metrics
- **Request Rate**: Requests per second
- **Response Time**: Average response time
- **Error Rate**: Percentage of failed requests
- **Active Connections**: Current active connections
- **Queue Length**: Pending requests in queue

### Performance Dashboard
```bash
# Access monitoring dashboard
http://localhost:8000/monitor
```

### Metrics Endpoint
```bash
curl -X GET "http://localhost:8000/metrics"
```

**Response:**
```json
{
  "requests_per_second": 2.5,
  "average_response_time": 45.2,
  "error_rate": 0.02,
  "active_connections": 12,
  "queue_length": 3,
  "uptime": 3600
}
```

## üß™ Testing

### API Testing
```bash
# Health check
curl -X GET "http://localhost:8000/health"

# Get combos
curl -X GET "http://localhost:8000/combos"

# Process test file
curl -X POST "http://localhost:8000/process-files" \
  -F "strategy=direct_file" \
  -F "provider=claude" \
  -F "files=@test_document.pdf"
```

### Load Testing
```bash
# Using Apache Bench
ab -n 100 -c 10 http://localhost:8000/health

# Using wrk
wrk -t12 -c400 -d30s http://localhost:8000/health
```

## üîí Security

### Authentication
```python
# API Key Authentication
API_KEY_HEADER = "X-API-Key"
API_KEYS = ["key1", "key2", "key3"]
```

### Rate Limiting
```python
# Rate limiting configuration
RATE_LIMIT_PER_MINUTE = 100
RATE_LIMIT_PER_HOUR = 1000
```

### CORS Configuration
```python
# CORS settings
CORS_ORIGINS = [
    "http://localhost:3000",
    "https://yourdomain.com"
]
```

## üêõ Error Handling

### Error Response Format
```json
{
  "error": {
    "code": "VALIDATION_ERROR",
    "message": "Invalid file format",
    "details": {
      "field": "files",
      "issue": "Unsupported file type"
    }
  },
  "timestamp": "2024-01-15T10:30:00Z",
  "request_id": "req_123456789"
}
```

### Common Error Codes
| Code | Description | HTTP Status |
|------|-------------|-------------|
| `VALIDATION_ERROR` | Invalid request parameters | 400 |
| `FILE_TOO_LARGE` | File exceeds size limit | 413 |
| `UNSUPPORTED_FORMAT` | Unsupported file format | 400 |
| `PROCESSING_ERROR` | Document processing failed | 500 |
| `TIMEOUT_ERROR` | Request timeout | 408 |
| `RATE_LIMIT_EXCEEDED` | Too many requests | 429 |

## üìä Performance Benchmarks

| Metric | Value | Notes |
|--------|-------|-------|
| **Requests/sec** | 50 | Single worker |
| **Concurrent Users** | 100 | 4 workers |
| **Response Time** | 45ms | Health check |
| **File Processing** | 30s | 1MB PDF |
| **Memory Usage** | 200MB | Base usage |
| **CPU Usage** | 15% | Idle state |

## üîß Development

### Adding New Endpoints
1. Define endpoint in `server.py`
2. Create request/response models
3. Implement business logic
4. Add error handling
5. Write tests

### Custom Middleware
```python
# Add custom middleware
@app.middleware("http")
async def add_process_time_header(request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    return response
```

### Environment Variables
```bash
# Server configuration
HOST=0.0.0.0
PORT=8000
DEBUG=false
WORKERS=4

# API configuration
API_RATE_LIMIT=100
API_TIMEOUT=300
```

## üìù Logging

### Log Format
```
2024-01-15 10:30:15.123 - INFO - [ThreadPoolExecutor-0_0][process_combo] - Processing combo: benchmark_combo
2024-01-15 10:30:15.456 - INFO - [ThreadPoolExecutor-0_0][process_combo] - Files received: 2
2024-01-15 10:30:45.789 - INFO - [ThreadPoolExecutor-0_0][process_combo] - Processing completed in 30.6s
```

### Log Levels
- **DEBUG**: Detailed processing information
- **INFO**: General API operations
- **WARNING**: Non-critical issues
- **ERROR**: API errors and failures

## üöÄ Deployment

### Docker Deployment
```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["uvicorn", "server:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Production Deployment
```bash
# Using Gunicorn
gunicorn server:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000

# Using systemd
sudo systemctl start ultra-arena-restful
sudo systemctl enable ultra-arena-restful
```

---

For detailed API documentation, visit `http://localhost:8000/docs` when the server is running.
